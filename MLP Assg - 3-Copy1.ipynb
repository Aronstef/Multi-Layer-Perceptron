{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9155f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9fac476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>178</th>\n",
       "      <th>13</th>\n",
       "      <th>class_0</th>\n",
       "      <th>class_1</th>\n",
       "      <th>class_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14.23</th>\n",
       "      <th>1.71</th>\n",
       "      <th>2.43</th>\n",
       "      <th>15.6</th>\n",
       "      <th>127</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.06</th>\n",
       "      <th>0.28</th>\n",
       "      <th>2.29</th>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.20</th>\n",
       "      <th>1.78</th>\n",
       "      <th>2.14</th>\n",
       "      <th>11.2</th>\n",
       "      <th>100</th>\n",
       "      <th>2.65</th>\n",
       "      <th>2.76</th>\n",
       "      <th>0.26</th>\n",
       "      <th>1.28</th>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.16</th>\n",
       "      <th>2.36</th>\n",
       "      <th>2.67</th>\n",
       "      <th>18.6</th>\n",
       "      <th>101</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.24</th>\n",
       "      <th>0.30</th>\n",
       "      <th>2.81</th>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.37</th>\n",
       "      <th>1.95</th>\n",
       "      <th>2.50</th>\n",
       "      <th>16.8</th>\n",
       "      <th>113</th>\n",
       "      <th>3.85</th>\n",
       "      <th>3.49</th>\n",
       "      <th>0.24</th>\n",
       "      <th>2.18</th>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.24</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.87</th>\n",
       "      <th>21.0</th>\n",
       "      <th>118</th>\n",
       "      <th>2.80</th>\n",
       "      <th>2.69</th>\n",
       "      <th>0.39</th>\n",
       "      <th>1.82</th>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.71</th>\n",
       "      <th>5.65</th>\n",
       "      <th>2.45</th>\n",
       "      <th>20.5</th>\n",
       "      <th>95</th>\n",
       "      <th>1.68</th>\n",
       "      <th>0.61</th>\n",
       "      <th>0.52</th>\n",
       "      <th>1.06</th>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.40</th>\n",
       "      <th>3.91</th>\n",
       "      <th>2.48</th>\n",
       "      <th>23.0</th>\n",
       "      <th>102</th>\n",
       "      <th>1.80</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.43</th>\n",
       "      <th>1.41</th>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.27</th>\n",
       "      <th>4.28</th>\n",
       "      <th>2.26</th>\n",
       "      <th>20.0</th>\n",
       "      <th>120</th>\n",
       "      <th>1.59</th>\n",
       "      <th>0.69</th>\n",
       "      <th>0.43</th>\n",
       "      <th>1.35</th>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.17</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.37</th>\n",
       "      <th>20.0</th>\n",
       "      <th>120</th>\n",
       "      <th>1.65</th>\n",
       "      <th>0.68</th>\n",
       "      <th>0.53</th>\n",
       "      <th>1.46</th>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.13</th>\n",
       "      <th>4.10</th>\n",
       "      <th>2.74</th>\n",
       "      <th>24.5</th>\n",
       "      <th>96</th>\n",
       "      <th>2.05</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0.56</th>\n",
       "      <th>1.35</th>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                178    13  class_0  class_1  \\\n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29   5.64  1.04     3.92     1065   \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28   4.38  1.05     3.40     1050   \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81   5.68  1.03     3.17     1185   \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18   7.80  0.86     3.45     1480   \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82   4.32  1.04     2.93      735   \n",
       "...                                             ...   ...      ...      ...   \n",
       "13.71 5.65 2.45 20.5 95  1.68 0.61 0.52 1.06   7.70  0.64     1.74      740   \n",
       "13.40 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41   7.30  0.70     1.56      750   \n",
       "13.27 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35  10.20  0.59     1.56      835   \n",
       "13.17 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46   9.30  0.60     1.62      840   \n",
       "14.13 4.10 2.74 24.5 96  2.05 0.76 0.56 1.35   9.20  0.61     1.60      560   \n",
       "\n",
       "                                              class_2  \n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29        0  \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28        0  \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81        0  \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18        0  \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82        0  \n",
       "...                                               ...  \n",
       "13.71 5.65 2.45 20.5 95  1.68 0.61 0.52 1.06        2  \n",
       "13.40 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41        2  \n",
       "13.27 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35        2  \n",
       "13.17 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46        2  \n",
       "14.13 4.10 2.74 24.5 96  2.05 0.76 0.56 1.35        2  \n",
       "\n",
       "[178 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Avita\\Downloads\\wine_data (1).csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de6b1de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\avita\\anaconda3\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\avita\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\avita\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\avita\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\avita\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9588eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['column_1','column_2','column_3','column_4','Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "053e4b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns= column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d0211cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['column_1', 'column_2', 'column_3', 'column_4', 'Target'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7b245cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_1    float64\n",
       "column_2    float64\n",
       "column_3    float64\n",
       "column_4      int64\n",
       "Target        int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c4dc30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14.23</th>\n",
       "      <th>1.71</th>\n",
       "      <th>2.43</th>\n",
       "      <th>15.6</th>\n",
       "      <th>127</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.06</th>\n",
       "      <th>0.28</th>\n",
       "      <th>2.29</th>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.20</th>\n",
       "      <th>1.78</th>\n",
       "      <th>2.14</th>\n",
       "      <th>11.2</th>\n",
       "      <th>100</th>\n",
       "      <th>2.65</th>\n",
       "      <th>2.76</th>\n",
       "      <th>0.26</th>\n",
       "      <th>1.28</th>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.16</th>\n",
       "      <th>2.36</th>\n",
       "      <th>2.67</th>\n",
       "      <th>18.6</th>\n",
       "      <th>101</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.24</th>\n",
       "      <th>0.30</th>\n",
       "      <th>2.81</th>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.37</th>\n",
       "      <th>1.95</th>\n",
       "      <th>2.50</th>\n",
       "      <th>16.8</th>\n",
       "      <th>113</th>\n",
       "      <th>3.85</th>\n",
       "      <th>3.49</th>\n",
       "      <th>0.24</th>\n",
       "      <th>2.18</th>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.24</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.87</th>\n",
       "      <th>21.0</th>\n",
       "      <th>118</th>\n",
       "      <th>2.80</th>\n",
       "      <th>2.69</th>\n",
       "      <th>0.39</th>\n",
       "      <th>1.82</th>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              column_1  column_2  column_3  \\\n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29      5.64      1.04      3.92   \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28      4.38      1.05      3.40   \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81      5.68      1.03      3.17   \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18      7.80      0.86      3.45   \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82      4.32      1.04      2.93   \n",
       "\n",
       "                                              column_4  Target  \n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29      1065       0  \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28      1050       0  \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81      1185       0  \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18      1480       0  \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82       735       0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5d5140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7387572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5d62f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_standardised = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30dce90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate the mean and standard deviation of your data\n",
    "mean = np.mean(df, axis=0)\n",
    "std = np.std(df, axis=0)\n",
    "\n",
    "# Standardize the data\n",
    "X_standardized = (df- mean) / std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aea80eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14.23</th>\n",
       "      <th>1.71</th>\n",
       "      <th>2.43</th>\n",
       "      <th>15.6</th>\n",
       "      <th>127</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.06</th>\n",
       "      <th>0.28</th>\n",
       "      <th>2.29</th>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.20</th>\n",
       "      <th>1.78</th>\n",
       "      <th>2.14</th>\n",
       "      <th>11.2</th>\n",
       "      <th>100</th>\n",
       "      <th>2.65</th>\n",
       "      <th>2.76</th>\n",
       "      <th>0.26</th>\n",
       "      <th>1.28</th>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.16</th>\n",
       "      <th>2.36</th>\n",
       "      <th>2.67</th>\n",
       "      <th>18.6</th>\n",
       "      <th>101</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.24</th>\n",
       "      <th>0.30</th>\n",
       "      <th>2.81</th>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.37</th>\n",
       "      <th>1.95</th>\n",
       "      <th>2.50</th>\n",
       "      <th>16.8</th>\n",
       "      <th>113</th>\n",
       "      <th>3.85</th>\n",
       "      <th>3.49</th>\n",
       "      <th>0.24</th>\n",
       "      <th>2.18</th>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.24</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.87</th>\n",
       "      <th>21.0</th>\n",
       "      <th>118</th>\n",
       "      <th>2.80</th>\n",
       "      <th>2.69</th>\n",
       "      <th>0.39</th>\n",
       "      <th>1.82</th>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.71</th>\n",
       "      <th>5.65</th>\n",
       "      <th>2.45</th>\n",
       "      <th>20.5</th>\n",
       "      <th>95</th>\n",
       "      <th>1.68</th>\n",
       "      <th>0.61</th>\n",
       "      <th>0.52</th>\n",
       "      <th>1.06</th>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.40</th>\n",
       "      <th>3.91</th>\n",
       "      <th>2.48</th>\n",
       "      <th>23.0</th>\n",
       "      <th>102</th>\n",
       "      <th>1.80</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.43</th>\n",
       "      <th>1.41</th>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.27</th>\n",
       "      <th>4.28</th>\n",
       "      <th>2.26</th>\n",
       "      <th>20.0</th>\n",
       "      <th>120</th>\n",
       "      <th>1.59</th>\n",
       "      <th>0.69</th>\n",
       "      <th>0.43</th>\n",
       "      <th>1.35</th>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.17</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.37</th>\n",
       "      <th>20.0</th>\n",
       "      <th>120</th>\n",
       "      <th>1.65</th>\n",
       "      <th>0.68</th>\n",
       "      <th>0.53</th>\n",
       "      <th>1.46</th>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.13</th>\n",
       "      <th>4.10</th>\n",
       "      <th>2.74</th>\n",
       "      <th>24.5</th>\n",
       "      <th>96</th>\n",
       "      <th>2.05</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0.56</th>\n",
       "      <th>1.35</th>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              column_1  column_2  column_3  \\\n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29      5.64      1.04      3.92   \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28      4.38      1.05      3.40   \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81      5.68      1.03      3.17   \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18      7.80      0.86      3.45   \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82      4.32      1.04      2.93   \n",
       "...                                                ...       ...       ...   \n",
       "13.71 5.65 2.45 20.5 95  1.68 0.61 0.52 1.06      7.70      0.64      1.74   \n",
       "13.40 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41      7.30      0.70      1.56   \n",
       "13.27 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35     10.20      0.59      1.56   \n",
       "13.17 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46      9.30      0.60      1.62   \n",
       "14.13 4.10 2.74 24.5 96  2.05 0.76 0.56 1.35      9.20      0.61      1.60   \n",
       "\n",
       "                                              column_4  Target  \n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29      1065       0  \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28      1050       0  \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81      1185       0  \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18      1480       0  \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82       735       0  \n",
       "...                                                ...     ...  \n",
       "13.71 5.65 2.45 20.5 95  1.68 0.61 0.52 1.06       740       2  \n",
       "13.40 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41       750       2  \n",
       "13.27 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35       835       2  \n",
       "13.17 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46       840       2  \n",
       "14.13 4.10 2.74 24.5 96  2.05 0.76 0.56 1.35       560       2  \n",
       "\n",
       "[178 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0e00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 178 entries, (14.23, 1.71, 2.43, 15.6, 127, 2.8, 3.06, 0.28, 2.29) to (14.13, 4.1, 2.74, 24.5, 96, 2.05, 0.76, 0.56, 1.35)\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   column_1  178 non-null    float64\n",
      " 1   column_2  178 non-null    float64\n",
      " 2   column_3  178 non-null    float64\n",
      " 3   column_4  178 non-null    int64  \n",
      " 4   Target    178 non-null    int64  \n",
      "dtypes: float64(3), int64(2)\n",
      "memory usage: 41.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f2e70dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_1    0\n",
       "column_2    0\n",
       "column_3    0\n",
       "column_4    0\n",
       "Target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4590013b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "column_1    0\n",
       "column_2    0\n",
       "column_3    0\n",
       "column_4    0\n",
       "Target      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfc6a4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "358e8d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(178, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "203c9ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>column_1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.521813</td>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.265668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_2</th>\n",
       "      <td>-0.521813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>0.236183</td>\n",
       "      <td>-0.617369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_3</th>\n",
       "      <td>-0.428815</td>\n",
       "      <td>0.565468</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312761</td>\n",
       "      <td>-0.788230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_4</th>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.236183</td>\n",
       "      <td>0.312761</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.633717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>0.265668</td>\n",
       "      <td>-0.617369</td>\n",
       "      <td>-0.788230</td>\n",
       "      <td>-0.633717</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          column_1  column_2  column_3  column_4    Target\n",
       "column_1  1.000000 -0.521813 -0.428815  0.316100  0.265668\n",
       "column_2 -0.521813  1.000000  0.565468  0.236183 -0.617369\n",
       "column_3 -0.428815  0.565468  1.000000  0.312761 -0.788230\n",
       "column_4  0.316100  0.236183  0.312761  1.000000 -0.633717\n",
       "Target    0.265668 -0.617369 -0.788230 -0.633717  1.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1baa10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.60    4\n",
       "4.60    4\n",
       "3.80    4\n",
       "3.40    3\n",
       "5.00    3\n",
       "       ..\n",
       "6.30    1\n",
       "7.05    1\n",
       "7.20    1\n",
       "8.90    1\n",
       "9.20    1\n",
       "Name: column_1, Length: 132, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['column_1'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a93bdc",
   "metadata": {},
   "source": [
    "# Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c6cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =df.drop(columns=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e65b463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16a49551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>column_1</th>\n",
       "      <th>column_2</th>\n",
       "      <th>column_3</th>\n",
       "      <th>column_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14.23</th>\n",
       "      <th>1.71</th>\n",
       "      <th>2.43</th>\n",
       "      <th>15.6</th>\n",
       "      <th>127</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.06</th>\n",
       "      <th>0.28</th>\n",
       "      <th>2.29</th>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.20</th>\n",
       "      <th>1.78</th>\n",
       "      <th>2.14</th>\n",
       "      <th>11.2</th>\n",
       "      <th>100</th>\n",
       "      <th>2.65</th>\n",
       "      <th>2.76</th>\n",
       "      <th>0.26</th>\n",
       "      <th>1.28</th>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.16</th>\n",
       "      <th>2.36</th>\n",
       "      <th>2.67</th>\n",
       "      <th>18.6</th>\n",
       "      <th>101</th>\n",
       "      <th>2.80</th>\n",
       "      <th>3.24</th>\n",
       "      <th>0.30</th>\n",
       "      <th>2.81</th>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.37</th>\n",
       "      <th>1.95</th>\n",
       "      <th>2.50</th>\n",
       "      <th>16.8</th>\n",
       "      <th>113</th>\n",
       "      <th>3.85</th>\n",
       "      <th>3.49</th>\n",
       "      <th>0.24</th>\n",
       "      <th>2.18</th>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.24</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.87</th>\n",
       "      <th>21.0</th>\n",
       "      <th>118</th>\n",
       "      <th>2.80</th>\n",
       "      <th>2.69</th>\n",
       "      <th>0.39</th>\n",
       "      <th>1.82</th>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.71</th>\n",
       "      <th>5.65</th>\n",
       "      <th>2.45</th>\n",
       "      <th>20.5</th>\n",
       "      <th>95</th>\n",
       "      <th>1.68</th>\n",
       "      <th>0.61</th>\n",
       "      <th>0.52</th>\n",
       "      <th>1.06</th>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.40</th>\n",
       "      <th>3.91</th>\n",
       "      <th>2.48</th>\n",
       "      <th>23.0</th>\n",
       "      <th>102</th>\n",
       "      <th>1.80</th>\n",
       "      <th>0.75</th>\n",
       "      <th>0.43</th>\n",
       "      <th>1.41</th>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.27</th>\n",
       "      <th>4.28</th>\n",
       "      <th>2.26</th>\n",
       "      <th>20.0</th>\n",
       "      <th>120</th>\n",
       "      <th>1.59</th>\n",
       "      <th>0.69</th>\n",
       "      <th>0.43</th>\n",
       "      <th>1.35</th>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13.17</th>\n",
       "      <th>2.59</th>\n",
       "      <th>2.37</th>\n",
       "      <th>20.0</th>\n",
       "      <th>120</th>\n",
       "      <th>1.65</th>\n",
       "      <th>0.68</th>\n",
       "      <th>0.53</th>\n",
       "      <th>1.46</th>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14.13</th>\n",
       "      <th>4.10</th>\n",
       "      <th>2.74</th>\n",
       "      <th>24.5</th>\n",
       "      <th>96</th>\n",
       "      <th>2.05</th>\n",
       "      <th>0.76</th>\n",
       "      <th>0.56</th>\n",
       "      <th>1.35</th>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              column_1  column_2  column_3  \\\n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29      5.64      1.04      3.92   \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28      4.38      1.05      3.40   \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81      5.68      1.03      3.17   \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18      7.80      0.86      3.45   \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82      4.32      1.04      2.93   \n",
       "...                                                ...       ...       ...   \n",
       "13.71 5.65 2.45 20.5 95  1.68 0.61 0.52 1.06      7.70      0.64      1.74   \n",
       "13.40 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41      7.30      0.70      1.56   \n",
       "13.27 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35     10.20      0.59      1.56   \n",
       "13.17 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46      9.30      0.60      1.62   \n",
       "14.13 4.10 2.74 24.5 96  2.05 0.76 0.56 1.35      9.20      0.61      1.60   \n",
       "\n",
       "                                              column_4  \n",
       "14.23 1.71 2.43 15.6 127 2.80 3.06 0.28 2.29      1065  \n",
       "13.20 1.78 2.14 11.2 100 2.65 2.76 0.26 1.28      1050  \n",
       "13.16 2.36 2.67 18.6 101 2.80 3.24 0.30 2.81      1185  \n",
       "14.37 1.95 2.50 16.8 113 3.85 3.49 0.24 2.18      1480  \n",
       "13.24 2.59 2.87 21.0 118 2.80 2.69 0.39 1.82       735  \n",
       "...                                                ...  \n",
       "13.71 5.65 2.45 20.5 95  1.68 0.61 0.52 1.06       740  \n",
       "13.40 3.91 2.48 23.0 102 1.80 0.75 0.43 1.41       750  \n",
       "13.27 4.28 2.26 20.0 120 1.59 0.69 0.43 1.35       835  \n",
       "13.17 2.59 2.37 20.0 120 1.65 0.68 0.53 1.46       840  \n",
       "14.13 4.10 2.74 24.5 96  2.05 0.76 0.56 1.35       560  \n",
       "\n",
       "[178 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb8ac196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.23  1.71  2.43  15.6  127  2.80  3.06  0.28  2.29    0\n",
       "13.20  1.78  2.14  11.2  100  2.65  2.76  0.26  1.28    0\n",
       "13.16  2.36  2.67  18.6  101  2.80  3.24  0.30  2.81    0\n",
       "14.37  1.95  2.50  16.8  113  3.85  3.49  0.24  2.18    0\n",
       "13.24  2.59  2.87  21.0  118  2.80  2.69  0.39  1.82    0\n",
       "                                                       ..\n",
       "13.71  5.65  2.45  20.5  95   1.68  0.61  0.52  1.06    2\n",
       "13.40  3.91  2.48  23.0  102  1.80  0.75  0.43  1.41    2\n",
       "13.27  4.28  2.26  20.0  120  1.59  0.69  0.43  1.35    2\n",
       "13.17  2.59  2.37  20.0  120  1.65  0.68  0.53  1.46    2\n",
       "14.13  4.10  2.74  24.5  96   2.05  0.76  0.56  1.35    2\n",
       "Name: Target, Length: 178, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4baab8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.33,random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bef3540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(119, 4)\n",
      "(59, 4)\n",
      "(119,)\n",
      "(59,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49efc6ee",
   "metadata": {},
   "source": [
    "# Neural Network using Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff82e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48fd3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Initialisation\n",
    "model = Sequential()\n",
    "model.add(Input(4,))\n",
    "#modeladd(Dense(units=1,activation='relu'))\n",
    "model.add(Dense(units=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "442e0d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model compiler\n",
    "model.compile(optimizer='rmsprop',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abc31e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "4/4 [==============================] - 1s 110ms/step - loss: 560097.5000 - val_loss: 586763.7500\n",
      "Epoch 2/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 551278.1250 - val_loss: 579547.6875\n",
      "Epoch 3/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 544840.7500 - val_loss: 573291.5000\n",
      "Epoch 4/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 539192.6875 - val_loss: 567756.2500\n",
      "Epoch 5/250\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 534085.9375 - val_loss: 562507.0625\n",
      "Epoch 6/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 529131.1875 - val_loss: 557279.6875\n",
      "Epoch 7/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 524197.2188 - val_loss: 552035.8750\n",
      "Epoch 8/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 519348.9688 - val_loss: 547119.9375\n",
      "Epoch 9/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 514754.0938 - val_loss: 542244.1250\n",
      "Epoch 10/250\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 510131.5625 - val_loss: 537418.3750\n",
      "Epoch 11/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 505591.9375 - val_loss: 532631.8750\n",
      "Epoch 12/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 501077.2188 - val_loss: 527848.1875\n",
      "Epoch 13/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 496586.7500 - val_loss: 523126.0938\n",
      "Epoch 14/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 492127.9375 - val_loss: 518347.0625\n",
      "Epoch 15/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 487661.3125 - val_loss: 513714.3125\n",
      "Epoch 16/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 483276.3125 - val_loss: 509123.4375\n",
      "Epoch 17/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 478919.5625 - val_loss: 504454.7500\n",
      "Epoch 18/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 474535.0625 - val_loss: 499860.4062\n",
      "Epoch 19/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 470188.9062 - val_loss: 495207.9375\n",
      "Epoch 20/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 465789.4688 - val_loss: 490549.6875\n",
      "Epoch 21/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 461409.5312 - val_loss: 485955.5938\n",
      "Epoch 22/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 457097.7812 - val_loss: 481404.7188\n",
      "Epoch 23/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 452803.6250 - val_loss: 476840.6250\n",
      "Epoch 24/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 448472.7188 - val_loss: 472207.9375\n",
      "Epoch 25/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 444161.0312 - val_loss: 467751.1875\n",
      "Epoch 26/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 439956.0938 - val_loss: 463318.5000\n",
      "Epoch 27/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 435749.1562 - val_loss: 458847.3125\n",
      "Epoch 28/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 431544.9375 - val_loss: 454418.7188\n",
      "Epoch 29/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 427366.2500 - val_loss: 450014.6875\n",
      "Epoch 30/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 423237.8125 - val_loss: 445677.1562\n",
      "Epoch 31/250\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 419119.5938 - val_loss: 441304.6250\n",
      "Epoch 32/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 414999.4062 - val_loss: 436859.0625\n",
      "Epoch 33/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 410829.0312 - val_loss: 432575.7500\n",
      "Epoch 34/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 406790.1875 - val_loss: 428271.3750\n",
      "Epoch 35/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 402719.8750 - val_loss: 423985.2188\n",
      "Epoch 36/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 398748.4375 - val_loss: 419841.9688\n",
      "Epoch 37/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 394751.9688 - val_loss: 415490.6562\n",
      "Epoch 38/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 390672.6250 - val_loss: 411235.0625\n",
      "Epoch 39/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 386690.7188 - val_loss: 407109.5625\n",
      "Epoch 40/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 382810.6875 - val_loss: 403019.1562\n",
      "Epoch 41/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 378917.4375 - val_loss: 398835.3750\n",
      "Epoch 42/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 375021.0312 - val_loss: 394693.5000\n",
      "Epoch 43/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 371085.1562 - val_loss: 390577.7500\n",
      "Epoch 44/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 367213.4375 - val_loss: 386461.4375\n",
      "Epoch 45/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 363386.5625 - val_loss: 382524.4062\n",
      "Epoch 46/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 359629.0312 - val_loss: 378505.8750\n",
      "Epoch 47/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 355859.5000 - val_loss: 374530.9688\n",
      "Epoch 48/250\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 352069.4688 - val_loss: 370481.8438\n",
      "Epoch 49/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 348303.0938 - val_loss: 366583.3125\n",
      "Epoch 50/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 344609.8125 - val_loss: 362603.6562\n",
      "Epoch 51/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 340831.3125 - val_loss: 358617.2812\n",
      "Epoch 52/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 337082.2812 - val_loss: 354602.3750\n",
      "Epoch 53/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 333352.4688 - val_loss: 350816.2812\n",
      "Epoch 54/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 329740.0938 - val_loss: 346937.5938\n",
      "Epoch 55/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 326112.9375 - val_loss: 343130.5625\n",
      "Epoch 56/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 322501.0625 - val_loss: 339309.0938\n",
      "Epoch 57/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 318904.9062 - val_loss: 335525.5000\n",
      "Epoch 58/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 315339.7500 - val_loss: 331718.2188\n",
      "Epoch 59/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 311772.9062 - val_loss: 328008.0000\n",
      "Epoch 60/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 308267.8125 - val_loss: 324294.3750\n",
      "Epoch 61/250\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 304726.1875 - val_loss: 320450.4375\n",
      "Epoch 62/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 301161.4375 - val_loss: 316830.7188\n",
      "Epoch 63/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 297741.8438 - val_loss: 313133.7500\n",
      "Epoch 64/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 294267.0312 - val_loss: 309535.1875\n",
      "Epoch 65/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 290832.1562 - val_loss: 305803.5938\n",
      "Epoch 66/250\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 287364.2500 - val_loss: 302212.5312\n",
      "Epoch 67/250\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 283997.3750 - val_loss: 298753.4375\n",
      "Epoch 68/250\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 280730.7500 - val_loss: 295251.3438\n",
      "Epoch 69/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 277381.9062 - val_loss: 291638.1562\n",
      "Epoch 70/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 274049.0000 - val_loss: 288222.3438\n",
      "Epoch 71/250\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 270771.2812 - val_loss: 284696.5312\n",
      "Epoch 72/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 267483.4062 - val_loss: 281290.4688\n",
      "Epoch 73/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 264242.6875 - val_loss: 277811.3125\n",
      "Epoch 74/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 260983.1562 - val_loss: 274382.4688\n",
      "Epoch 75/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 257741.6562 - val_loss: 270921.6250\n",
      "Epoch 76/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 254508.7031 - val_loss: 267563.9062\n",
      "Epoch 77/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 251339.7031 - val_loss: 264202.7812\n",
      "Epoch 78/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 248158.9531 - val_loss: 260857.9844\n",
      "Epoch 79/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 244984.6406 - val_loss: 257445.0156\n",
      "Epoch 80/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 241816.4531 - val_loss: 254181.8594\n",
      "Epoch 81/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 238733.4531 - val_loss: 250926.5625\n",
      "Epoch 82/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 235662.4062 - val_loss: 247696.5781\n",
      "Epoch 83/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 232617.7188 - val_loss: 244480.3438\n",
      "Epoch 84/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 229584.2500 - val_loss: 241269.8594\n",
      "Epoch 85/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 226555.3906 - val_loss: 238037.8125\n",
      "Epoch 86/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 223520.3438 - val_loss: 234863.7969\n",
      "Epoch 87/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 220554.9844 - val_loss: 231784.7188\n",
      "Epoch 88/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 217612.2344 - val_loss: 228579.2188\n",
      "Epoch 89/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 214606.4531 - val_loss: 225446.8438\n",
      "Epoch 90/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 211664.5938 - val_loss: 222377.3594\n",
      "Epoch 91/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 208776.8125 - val_loss: 219326.0312\n",
      "Epoch 92/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 205928.0156 - val_loss: 216349.8906\n",
      "Epoch 93/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 203090.0469 - val_loss: 213300.9688\n",
      "Epoch 94/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 200207.3438 - val_loss: 210253.5156\n",
      "Epoch 95/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 197374.4219 - val_loss: 207302.8438\n",
      "Epoch 96/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 194568.3125 - val_loss: 204311.6250\n",
      "Epoch 97/250\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 191759.4844 - val_loss: 201354.7500\n",
      "Epoch 98/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 188954.6250 - val_loss: 198349.0469\n",
      "Epoch 99/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 186156.3438 - val_loss: 195456.9531\n",
      "Epoch 100/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 183459.4219 - val_loss: 192668.5469\n",
      "Epoch 101/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 180789.8281 - val_loss: 189752.9688\n",
      "Epoch 102/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 178069.5469 - val_loss: 186914.6406\n",
      "Epoch 103/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 175402.6562 - val_loss: 184158.2344\n",
      "Epoch 104/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 172815.9375 - val_loss: 181432.4688\n",
      "Epoch 105/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 170209.5938 - val_loss: 178615.8438\n",
      "Epoch 106/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 167566.6875 - val_loss: 175845.6875\n",
      "Epoch 107/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 164971.7031 - val_loss: 173139.5156\n",
      "Epoch 108/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 162435.8281 - val_loss: 170478.6719\n",
      "Epoch 109/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 159924.9688 - val_loss: 167838.1406\n",
      "Epoch 110/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 157430.3750 - val_loss: 165153.7969\n",
      "Epoch 111/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 154901.6562 - val_loss: 162517.5781\n",
      "Epoch 112/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 152421.9844 - val_loss: 159892.5469\n",
      "Epoch 113/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 149938.7031 - val_loss: 157243.2500\n",
      "Epoch 114/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 147471.2344 - val_loss: 154714.0312\n",
      "Epoch 115/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 145073.7031 - val_loss: 152121.6094\n",
      "Epoch 116/250\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 142647.7500 - val_loss: 149611.0781\n",
      "Epoch 117/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 140291.3281 - val_loss: 147112.4375\n",
      "Epoch 118/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 137915.9219 - val_loss: 144583.3906\n",
      "Epoch 119/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 135571.7500 - val_loss: 142178.5625\n",
      "Epoch 120/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 133294.9688 - val_loss: 139756.9688\n",
      "Epoch 121/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 130994.7422 - val_loss: 137303.4375\n",
      "Epoch 122/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 128695.3203 - val_loss: 134880.1875\n",
      "Epoch 123/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 126418.0156 - val_loss: 132488.9844\n",
      "Epoch 124/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 124176.5703 - val_loss: 130147.9688\n",
      "Epoch 125/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 121954.9375 - val_loss: 127765.7812\n",
      "Epoch 126/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 119722.8594 - val_loss: 125421.3203\n",
      "Epoch 127/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 117535.1953 - val_loss: 123156.4062\n",
      "Epoch 128/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 115435.9141 - val_loss: 121001.8047\n",
      "Epoch 129/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 113379.2578 - val_loss: 118796.6641\n",
      "Epoch 130/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 111302.5781 - val_loss: 116591.6953\n",
      "Epoch 131/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 109211.5312 - val_loss: 114369.8672\n",
      "Epoch 132/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 107125.3438 - val_loss: 112183.7109\n",
      "Epoch 133/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 105089.1562 - val_loss: 110082.7812\n",
      "Epoch 134/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 103083.6016 - val_loss: 107906.6641\n",
      "Epoch 135/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 101053.8672 - val_loss: 105799.9766\n",
      "Epoch 136/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 99072.0859 - val_loss: 103706.6953\n",
      "Epoch 137/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 97101.9219 - val_loss: 101628.0000\n",
      "Epoch 138/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 95146.6562 - val_loss: 99558.1797\n",
      "Epoch 139/250\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 93194.5625 - val_loss: 97504.7266\n",
      "Epoch 140/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 91260.2422 - val_loss: 95434.3594\n",
      "Epoch 141/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 89337.7266 - val_loss: 93475.8125\n",
      "Epoch 142/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 87498.1719 - val_loss: 91545.0312\n",
      "Epoch 143/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 85672.7734 - val_loss: 89612.7812\n",
      "Epoch 144/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 83855.1953 - val_loss: 87701.1328\n",
      "Epoch 145/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 82073.9688 - val_loss: 85802.6797\n",
      "Epoch 146/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 80299.5547 - val_loss: 83991.3594\n",
      "Epoch 147/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 78570.4375 - val_loss: 82132.3359\n",
      "Epoch 148/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 76827.1250 - val_loss: 80318.0000\n",
      "Epoch 149/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 75118.1875 - val_loss: 78489.8125\n",
      "Epoch 150/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 73393.1953 - val_loss: 76683.9766\n",
      "Epoch 151/250\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 71685.6016 - val_loss: 74852.3359\n",
      "Epoch 152/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 69998.5703 - val_loss: 73141.8203\n",
      "Epoch 153/250\n",
      "4/4 [==============================] - 0s 31ms/step - loss: 68398.0234 - val_loss: 71444.8672\n",
      "Epoch 154/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 66784.7578 - val_loss: 69718.8359\n",
      "Epoch 155/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 65159.1016 - val_loss: 68016.8438\n",
      "Epoch 156/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 63555.7031 - val_loss: 66327.3281\n",
      "Epoch 157/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 61980.9688 - val_loss: 64704.9805\n",
      "Epoch 158/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 60450.2891 - val_loss: 63077.5977\n",
      "Epoch 159/250\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 58922.1602 - val_loss: 61468.4922\n",
      "Epoch 160/250\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 57413.2930 - val_loss: 59903.2656\n",
      "Epoch 161/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 55943.8828 - val_loss: 58346.2070\n",
      "Epoch 162/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 54481.2227 - val_loss: 56807.6016\n",
      "Epoch 163/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 53038.8555 - val_loss: 55313.4336\n",
      "Epoch 164/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 51625.4844 - val_loss: 53804.7891\n",
      "Epoch 165/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 50201.7656 - val_loss: 52278.6367\n",
      "Epoch 166/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 48802.2852 - val_loss: 50869.4492\n",
      "Epoch 167/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 47459.6992 - val_loss: 49441.1055\n",
      "Epoch 168/250\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 46112.1602 - val_loss: 48005.0273\n",
      "Epoch 169/250\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 44788.6367 - val_loss: 46666.6133\n",
      "Epoch 170/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 43508.1602 - val_loss: 45273.0156\n",
      "Epoch 171/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 42201.5312 - val_loss: 43903.3047\n",
      "Epoch 172/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 40929.6836 - val_loss: 42580.9688\n",
      "Epoch 173/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 39675.7812 - val_loss: 41250.7305\n",
      "Epoch 174/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 38446.7422 - val_loss: 39998.3555\n",
      "Epoch 175/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 37256.1250 - val_loss: 38725.0000\n",
      "Epoch 176/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 36077.5938 - val_loss: 37501.7461\n",
      "Epoch 177/250\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 34918.6523 - val_loss: 36276.0859\n",
      "Epoch 178/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 33779.6680 - val_loss: 35089.9102\n",
      "Epoch 179/250\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 32653.3438 - val_loss: 33883.1328\n",
      "Epoch 180/250\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 31521.1855 - val_loss: 32708.3594\n",
      "Epoch 181/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 30431.6211 - val_loss: 31582.7754\n",
      "Epoch 182/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 29377.7051 - val_loss: 30476.7227\n",
      "Epoch 183/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 28335.3691 - val_loss: 29385.6699\n",
      "Epoch 184/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 27314.0449 - val_loss: 28301.7930\n",
      "Epoch 185/250\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 26296.8066 - val_loss: 27247.1934\n",
      "Epoch 186/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 25307.8945 - val_loss: 26210.9707\n",
      "Epoch 187/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 24334.6309 - val_loss: 25184.9512\n",
      "Epoch 188/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 23377.6504 - val_loss: 24181.0176\n",
      "Epoch 189/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 22442.9629 - val_loss: 23211.4980\n",
      "Epoch 190/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 21536.6934 - val_loss: 22257.5801\n",
      "Epoch 191/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 20641.3359 - val_loss: 21318.8496\n",
      "Epoch 192/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 19760.9746 - val_loss: 20393.1953\n",
      "Epoch 193/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 18908.9941 - val_loss: 19546.7969\n",
      "Epoch 194/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 18110.6816 - val_loss: 18690.1230\n",
      "Epoch 195/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 17304.6133 - val_loss: 17845.4551\n",
      "Epoch 196/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 16517.5312 - val_loss: 17027.0078\n",
      "Epoch 197/250\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 15753.5781 - val_loss: 16234.3154\n",
      "Epoch 198/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 15004.0977 - val_loss: 15435.9424\n",
      "Epoch 199/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 14263.6650 - val_loss: 14668.5947\n",
      "Epoch 200/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 13544.6660 - val_loss: 13915.9961\n",
      "Epoch 201/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 12841.0049 - val_loss: 13174.6289\n",
      "Epoch 202/250\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 12152.5586 - val_loss: 12467.8350\n",
      "Epoch 203/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 11497.8740 - val_loss: 11801.8018\n",
      "Epoch 204/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 10873.7178 - val_loss: 11140.3486\n",
      "Epoch 205/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 10260.4189 - val_loss: 10511.0957\n",
      "Epoch 206/250\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 9668.9463 - val_loss: 9883.7246\n",
      "Epoch 207/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 9080.9424 - val_loss: 9269.1953\n",
      "Epoch 208/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 8513.2754 - val_loss: 8687.2803\n",
      "Epoch 209/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 7970.9214 - val_loss: 8123.9482\n",
      "Epoch 210/250\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 7444.9263 - val_loss: 7574.7300\n",
      "Epoch 211/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 6940.9385 - val_loss: 7062.6729\n",
      "Epoch 212/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 6463.6338 - val_loss: 6563.4863\n",
      "Epoch 213/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 6000.7275 - val_loss: 6093.2271\n",
      "Epoch 214/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 5562.5776 - val_loss: 5630.2437\n",
      "Epoch 215/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 5130.3940 - val_loss: 5183.3208\n",
      "Epoch 216/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 4718.2061 - val_loss: 4758.6816\n",
      "Epoch 217/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 4326.3765 - val_loss: 4355.5161\n",
      "Epoch 218/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 3953.9409 - val_loss: 3979.4358\n",
      "Epoch 219/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 3604.4438 - val_loss: 3610.7649\n",
      "Epoch 220/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 3264.7380 - val_loss: 3269.9971\n",
      "Epoch 221/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2950.4465 - val_loss: 2945.3872\n",
      "Epoch 222/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2649.5107 - val_loss: 2632.9238\n",
      "Epoch 223/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2364.1318 - val_loss: 2348.3501\n",
      "Epoch 224/250\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2102.1238 - val_loss: 2079.0737\n",
      "Epoch 225/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1856.1261 - val_loss: 1831.2924\n",
      "Epoch 226/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1630.5389 - val_loss: 1602.6030\n",
      "Epoch 227/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1419.7015 - val_loss: 1388.9305\n",
      "Epoch 228/250\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1224.0247 - val_loss: 1185.5204\n",
      "Epoch 229/250\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1040.2777 - val_loss: 1006.5142\n",
      "Epoch 230/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 879.0353 - val_loss: 847.2911\n",
      "Epoch 231/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 735.1659 - val_loss: 703.9937\n",
      "Epoch 232/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 606.7642 - val_loss: 578.6458\n",
      "Epoch 233/250\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 493.9076 - val_loss: 463.1772\n",
      "Epoch 234/250\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 391.8346 - val_loss: 367.5616\n",
      "Epoch 235/250\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 307.6783 - val_loss: 287.2245\n",
      "Epoch 236/250\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 236.8639 - val_loss: 217.7149\n",
      "Epoch 237/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 177.2550 - val_loss: 162.0580\n",
      "Epoch 238/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 130.3367 - val_loss: 121.4656\n",
      "Epoch 239/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 96.2959 - val_loss: 91.4827\n",
      "Epoch 240/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 71.6339 - val_loss: 69.9408\n",
      "Epoch 241/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 53.8262 - val_loss: 53.8698\n",
      "Epoch 242/250\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 41.6200 - val_loss: 43.9810\n",
      "Epoch 243/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 34.7884 - val_loss: 39.0534\n",
      "Epoch 244/250\n",
      "4/4 [==============================] - 0s 26ms/step - loss: 31.1283 - val_loss: 36.3462\n",
      "Epoch 245/250\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 29.5154 - val_loss: 35.2836\n",
      "Epoch 246/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 28.9704 - val_loss: 34.6822\n",
      "Epoch 247/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 28.9421 - val_loss: 34.5183\n",
      "Epoch 248/250\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 29.2552 - val_loss: 34.5812\n",
      "Epoch 249/250\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 28.7433 - val_loss: 34.4473\n",
      "Epoch 250/250\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 28.8077 - val_loss: 34.6271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a320caaf10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training\n",
    "model.fit(x=x_train,y=y_train,epochs=250,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48c31c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "92c8c209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x1 = [15,20,25]\n",
    "x2 = [8,12,15]\n",
    "y=[25,30,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3cf81d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21120\\3814061550.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"initialied Weights : w1:{w1}|w2:{w2}|b:{b}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "random.init= np.random.randn(3)\n",
    "w1,w2,b = random.init[0],random.init[1], random.init[2]\n",
    "print(f\"initialied Weights : w1:{w1}|w2:{w2}|b:{b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496b736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(w1,w2,b):\n",
    "y_pred=[]\n",
    "Total_error = 0\n",
    "\n",
    " for (i) in range(n) :\n",
    "        y_hat =w1 * x1[i] + w2 * x2[i]+ b \n",
    "        y_pred.append(y_hat)\n",
    "    \n",
    "    E = y_hat-y(i)**2\n",
    "    Total_error += E\n",
    "    \n",
    "    return y_pred,Total_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a26fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred  = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e7b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5cbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred,Total_error = forward_pass(w1,w2,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a546cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred,Total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "mae = mean_absolute_error(Total_error, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f048bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f629c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
